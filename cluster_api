from flask import Flask, request, jsonify
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score
from kmodes.kprototypes import KPrototypes
from sklearn.cluster import DBSCAN
import os

app = Flask(__name__)

def preprocess_data(file_path):
    """
    Load the file and preprocess the data.
    """
    if not os.path.exists(file_path):
        return None, "File not found."

    try:
        df = pd.read_csv(file_path)
    except Exception as e:
        return None, f"Error reading file: {str(e)}"

    return df, None

def perform_k_prototypes_clustering(df, max_no_of_clusters):
    """
    Perform K-Prototypes clustering.
    """
    numerical_columns = df.select_dtypes(include=['float64', 'int']).columns.tolist()
    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()

    # Standardize numerical data
    scaler = StandardScaler()
    scaled_numerical_data = scaler.fit_transform(df[numerical_columns])

    # Combine scaled numerical and categorical data
    processed_data = pd.concat(
        [
            pd.DataFrame(scaled_numerical_data, columns=numerical_columns),
            df[categorical_columns].reset_index(drop=True),
        ],
        axis=1,
    )

    # Convert categorical columns to numeric encodings
    for col in categorical_columns:
        processed_data[col] = processed_data[col].astype('category').cat.codes

    # Find optimal clusters
    silhouette_scores = []
    for n_clusters in range(2, max_no_of_clusters + 1):
        kproto = KPrototypes(n_clusters=n_clusters, random_state=42)
        cluster_labels = kproto.fit_predict(
            processed_data, categorical=list(range(len(numerical_columns), processed_data.shape[1]))
        )

        silhouette_avg = silhouette_score(scaled_numerical_data, cluster_labels)
        silhouette_scores.append(silhouette_avg)

    optimal_clusters = range(2, max_no_of_clusters + 1)[silhouette_scores.index(max(silhouette_scores))]

    # Perform clustering with optimal clusters
    kproto = KPrototypes(n_clusters=optimal_clusters, random_state=42)
    df['Cluster'] = kproto.fit_predict(
        processed_data, categorical=list(range(len(numerical_columns), processed_data.shape[1]))
    )

    return df, optimal_clusters

def perform_dbscan_clustering(df):
    """
    Perform DBSCAN clustering.
    """
    numerical_columns = df.select_dtypes(include=['float64', 'int']).columns.tolist()

    # Standardize numerical data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(df[numerical_columns])

    best_db_score = float('inf')
    best_eps, best_min_samples = None, None
    best_cluster_labels = None

    eps_values = [0.2, 0.3, 0.4]
    min_samples_values = [3, 5, 10]

    for eps in eps_values:
        for min_samples in min_samples_values:
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            cluster_labels = dbscan.fit_predict(scaled_data)

            if len(set(cluster_labels)) < 2:
                continue

            db_score = davies_bouldin_score(scaled_data, cluster_labels)

            if db_score < best_db_score:
                best_db_score = db_score
                best_eps, best_min_samples = eps, min_samples
                best_cluster_labels = cluster_labels

    df['Cluster'] = best_cluster_labels
    return df, best_eps, best_min_samples

@app.route('/cluster', methods=['POST'])
def cluster():
    """
    API endpoint to perform clustering.
    """
    user_input = request.json
    file_path = user_input.get('file_path')
    max_no_of_clusters = user_input.get('max_no_of_clusters')
    algorithm_name = user_input.get('algorithm_name')

    if not file_path or not max_no_of_clusters or not algorithm_name:
        return jsonify({'error': 'Missing required parameters: file_path, max_no_of_clusters, algorithm_name'}), 400

    # Preprocess data
    df, error = preprocess_data(file_path)
    if error:
        return jsonify({'error': error}), 400

    if algorithm_name.lower() == 'k-prototypes':
        clustered_data, optimal_clusters = perform_k_prototypes_clustering(df, max_no_of_clusters)
        return jsonify({
            'message': 'K-Prototypes clustering completed successfully',
            'optimal_clusters': optimal_clusters,
            'data_preview': clustered_data.head().to_dict(orient='records'),
        })
    elif algorithm_name.lower() == 'dbscan':
        clustered_data, best_eps, best_min_samples = perform_dbscan_clustering(df)
        return jsonify({
            'message': 'DBSCAN clustering completed successfully',
            'best_eps': best_eps,
            'best_min_samples': best_min_samples,
            'data_preview': clustered_data.head().to_dict(orient='records'),
        })
    else:
        return jsonify({'error': 'Invalid algorithm_name. Use "k-prototypes" or "dbscan".'}), 400

if __name__ == '__main__':
    app.run(debug=True)
